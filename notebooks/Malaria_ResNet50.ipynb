{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Malaria Blood Cell Classification with ResNet-50 (TensorFlow/Keras)\n",
    "\n",
    "This notebook trains and evaluates a ResNet-50 model to classify blood cell images as Parasitized vs Uninfected using a manifest-based `tf.data` pipeline (no image copying).\n",
    "\n",
    "- Dataset path: `/Users/jitesh/Downloads/cell_images` (Parasitized/ and Uninfected/)\n",
    "- Manifests: `data/manifests/train.csv`, `val.csv`, `test.csv`\n",
    "- Image size: 224, Batch size: 32\n",
    "- Two-phase training: head, then fine-tune last ResNet block\n",
    "- Metrics: Accuracy, Precision, Recall, F1, ROC-AUC; Confusion Matrix saved to `reports/figures/confusion_matrix.png`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jitesh/Malaria-Blood-Cell-Classification-Using-ResNet-50/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF: 2.16.2\n",
      "Devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "ROOT: /Users/jitesh/Malaria-Blood-Cell-Classification-Using-ResNet-50\n"
     ]
    }
   ],
   "source": [
    "# leave these commented\n",
    "# %pip install --upgrade pip\n",
    "# %pip install -r ../requirements.txt\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- Add project root (parent of notebooks/) to Python path ---\n",
    "ROOT = Path(\"..\").resolve()\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(ROOT))\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "# NumPy <-> TensorFlow compatibility shim\n",
    "if not hasattr(np, \"complex_\"):\n",
    "    np.complex_ = np.complex128\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "from src.datasets_tf import get_datasets_from_manifests\n",
    "from src.model_tf import build_resnet50\n",
    "\n",
    "print(\"TF:\", tf.__version__)\n",
    "print(\"Devices:\", tf.config.list_physical_devices())\n",
    "print(\"ROOT:\", ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DIR = Path('/Users/jitesh/Downloads/cell_images')\n",
    "MANIFEST_DIR = Path('../data/manifests')\n",
    "MODELS_DIR = Path('../models')\n",
    "REPORTS_DIR = Path('../reports')\n",
    "FIG_DIR = REPORTS_DIR / 'figures'\n",
    "\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS_HEAD = 10\n",
    "EPOCHS_FT = 10\n",
    "LR_HEAD = 1e-4\n",
    "LR_FT = 1e-5\n",
    "PATIENCE = 4\n",
    "SEED = 42\n",
    "\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MODEL_OUT = MODELS_DIR / 'best_resnet50.h5'\n",
    "METRICS_TXT = REPORTS_DIR / 'metrics.txt'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create manifests (if missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manifests already exist at ../data/manifests\n"
     ]
    }
   ],
   "source": [
    "# Generate manifests only if they do not exist yet.\n",
    "if not (MANIFEST_DIR / 'train.csv').exists():\n",
    "    MANIFEST_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    import subprocess, sys\n",
    "    print('Creating manifests...')\n",
    "    cmd = [sys.executable, '-m', 'src.create_manifests', '--raw_dir', str(RAW_DIR), '--out_dir', str(MANIFEST_DIR), '--val_size', '0.15', '--test_size', '0.15', '--seed', str(SEED)]\n",
    "    print(' '.join(cmd))\n",
    "    res = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    print(res.stdout)\n",
    "    if res.returncode != 0:\n",
    "        print(res.stderr)\n",
    "        raise RuntimeError('Failed to create manifests')\n",
    "else:\n",
    "    print('Manifests already exist at', MANIFEST_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build datasets from manifests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-14 17:58:42.854862: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2025-11-14 17:58:42.855043: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2025-11-14 17:58:42.855046: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2025-11-14 17:58:42.855439: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-11-14 17:58:42.855449: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Parasitized', 'Uninfected']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds, val_ds, test_ds, class_names = get_datasets_from_manifests(str(MANIFEST_DIR), img_size=IMG_SIZE, batch_size=BATCH_SIZE, seed=SEED)\n",
    "class_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model and train (head phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-14 17:58:47.835996: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n",
      "2025-11-14 17:59:00.575252: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:10: Filling up shuffle buffer (this may take a while): 553 of 1000\n",
      "2025-11-14 17:59:01.725491: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:480] Shuffle buffer filled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 325ms/step - accuracy: 0.7480 - auc: 0.8246 - loss: 0.6496"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 408ms/step - accuracy: 0.7481 - auc: 0.8247 - loss: 0.6493 - val_accuracy: 0.8882 - val_auc: 0.9713 - val_loss: 0.2679 - learning_rate: 1.0000e-04\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-14 18:03:24.612981: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:10: Filling up shuffle buffer (this may take a while): 410 of 1000\n",
      "2025-11-14 18:03:35.144339: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:10: Filling up shuffle buffer (this may take a while): 548 of 1000\n",
      "2025-11-14 18:03:38.135295: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:480] Shuffle buffer filled.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Stage 1: train classification head only\n",
    "model, base = build_resnet50(input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(LR_HEAD),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_auc', mode='max', patience=PATIENCE, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_auc', mode='max', factor=0.5, patience=max(1, PATIENCE-1), min_lr=1e-6),\n",
    "    ModelCheckpoint(filepath=str(MODEL_OUT), monitor='val_auc', mode='max', save_best_only=True)\n",
    "]\n",
    "\n",
    "history_head = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS_HEAD, callbacks=callbacks)\n",
    "MODEL_OUT.exists(), MODEL_OUT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune last ResNet block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2: extended fine-tuning of upper ResNet blocks\n",
    "\n",
    "# Unfreeze all layers from conv4_block1_* onward (conv4 + conv5 blocks)\n",
    "base.trainable = True\n",
    "start_unfreeze = False\n",
    "for layer in base.layers:\n",
    "    name = layer.name\n",
    "    if 'conv4_block1' in name:\n",
    "        start_unfreeze = True\n",
    "    layer.trainable = start_unfreeze\n",
    "\n",
    "# Use a smaller learning rate for fine-tuning\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(LR_FT),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])\n",
    "\n",
    "history_ft = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS_FT, callbacks=callbacks)\n",
    "model.save(MODEL_OUT)\n",
    "MODEL_OUT.exists(), MODEL_OUT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_metrics = model.evaluate(val_ds, return_dict=True)\n",
    "test_metrics = model.evaluate(test_ds, return_dict=True)\n",
    "val_metrics, test_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed metrics and confusion matrix (Test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect ground truth and predictions\n",
    "y_true = []\n",
    "y_prob = []\n",
    "for batch, labels in test_ds:\n",
    "    y_true.extend(labels.numpy().reshape(-1).astype(int).tolist())\n",
    "    y_prob.extend(model.predict(batch, verbose=0).reshape(-1).tolist())\n",
    "\n",
    "y_true = np.array(y_true)\n",
    "y_prob = np.array(y_prob)\n",
    "y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "try:\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "except Exception:\n",
    "    auc = float('nan')\n",
    "\n",
    "print({'accuracy': acc, 'precision': prec, 'recall': rec, 'f1': f1, 'auc': auc})\n",
    "\n",
    "# Save metrics\n",
    "REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "with open(METRICS_TXT, 'w') as f:\n",
    "    f.write(f'Accuracy: {acc}\n",
    "')\n",
    "    f.write(f'Precision: {prec}\n",
    "')\n",
    "    f.write(f'Recall: {rec}\n",
    "')\n",
    "    f.write(f'F1: {f1}\n",
    "')\n",
    "    f.write(f'ROC-AUC: {auc}\n",
    "')\n",
    "print('Saved metrics to', METRICS_TXT)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(4,3))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=['Parasitized','Uninfected'],\n",
    "            yticklabels=['Parasitized','Uninfected'])\n",
    "plt.ylabel('True')\n",
    "plt.xlabel('Predicted')\n",
    "plt.tight_layout()\n",
    "fig_path = FIG_DIR / 'confusion_matrix.png'\n",
    "plt.savefig(fig_path, dpi=150)\n",
    "plt.show()\n",
    "print('Saved confusion matrix to', fig_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6866ce",
   "metadata": {},
   "source": [
    "## Training curves (loss and AUC)\n",
    "\n",
    "Visualize how training and validation metrics evolve over epochs for the head-training and fine-tuning stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c35cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves for head and fine-tuning phases\n",
    "\n",
    "def _plot_history(ax, history, metric, label_prefix):\n",
    "    if history is None:\n",
    "        return\n",
    "    values = history.history.get(metric)\n",
    "    val_values = history.history.get('val_' + metric)\n",
    "    if values is None:\n",
    "        return\n",
    "    epochs = range(1, len(values) + 1)\n",
    "    ax.plot(epochs, values, label=f'{label_prefix} train')\n",
    "    if val_values is not None:\n",
    "        ax.plot(epochs, val_values, label=f'{label_prefix} val')\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "# Loss\n",
    "_plot_history(axes[0], history_head, 'loss', 'head')\n",
    "_plot_history(axes[0], history_ft, 'loss', 'ft')\n",
    "axes[0].set_title('Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "\n",
    "# AUC\n",
    "_plot_history(axes[1], history_head, 'auc', 'head')\n",
    "_plot_history(axes[1], history_ft, 'auc', 'ft')\n",
    "axes[1].set_title('AUC')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('AUC')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93213b6e",
   "metadata": {},
   "source": [
    "## Export model to TFLite\n",
    "\n",
    "Convert the trained Keras model (`best_resnet50.h5`) into a TensorFlow Lite model for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e153bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the trained Keras model to TFLite\n",
    "\n",
    "keras_model_path = MODEL_OUT\n",
    "assert keras_model_path.exists(), f\"Model not found at {keras_model_path}\"\n",
    "\n",
    "tflite_model_path = MODELS_DIR / 'best_resnet50.tflite'\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(tf.keras.models.load_model(keras_model_path))\n",
    "# Enable float16 quantization for smaller size (optional)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_types = [tf.float16]\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "with open(tflite_model_path, 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print('Saved TFLite model to', tflite_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179201c7",
   "metadata": {},
   "source": [
    "## Single-image inference utility\n",
    "\n",
    "Use the trained Keras model (and optionally the TFLite model) to classify a single blood cell image by file path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c815696d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import preprocess_input as resnet_preprocess\n",
    "\n",
    "\n",
    "def load_and_preprocess_image(path, img_size=IMG_SIZE):\n",
    "    img_raw = tf.io.read_file(path)\n",
    "    img = tf.io.decode_image(img_raw, channels=3, expand_animations=False)\n",
    "    img = tf.image.resize(img, (img_size, img_size))\n",
    "    img = tf.cast(img, tf.float32)\n",
    "    img = resnet_preprocess(img[None, ...])  # add batch dim and preprocess\n",
    "    return img\n",
    "\n",
    "\n",
    "def predict_image_keras(model, path):\n",
    "    img = load_and_preprocess_image(path)\n",
    "    prob = float(model.predict(img, verbose=0)[0, 0])\n",
    "    label = 'Parasitized' if prob >= 0.5 else 'Uninfected'\n",
    "    return label, prob\n",
    "\n",
    "\n",
    "def predict_image_tflite(tflite_path, path):\n",
    "    interpreter = tf.lite.Interpreter(model_path=str(tflite_path))\n",
    "    interpreter.allocate_tensors()\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    img = load_and_preprocess_image(path)\n",
    "    # Cast to expected dtype\n",
    "    img = tf.cast(img, input_details[0]['dtype']).numpy()\n",
    "\n",
    "    interpreter.set_tensor(input_details[0]['index'], img)\n",
    "    interpreter.invoke()\n",
    "    output = interpreter.get_tensor(output_details[0]['index'])\n",
    "    prob = float(output[0][0])\n",
    "    label = 'Parasitized' if prob >= 0.5 else 'Uninfected'\n",
    "    return label, prob\n",
    "\n",
    "\n",
    "# Example usage (update `test_image_path` to a real image from your dataset)\n",
    "test_image_path = str(RAW_DIR / 'Parasitized' / os.listdir(RAW_DIR / 'Parasitized')[0])\n",
    "print('Test image:', test_image_path)\n",
    "\n",
    "keras_label, keras_prob = predict_image_keras(model, test_image_path)\n",
    "print('Keras model ->', keras_label, keras_prob)\n",
    "\n",
    "if (MODELS_DIR / 'best_resnet50.tflite').exists():\n",
    "    tflite_label, tflite_prob = predict_image_tflite(MODELS_DIR / 'best_resnet50.tflite', test_image_path)\n",
    "    print('TFLite model ->', tflite_label, tflite_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e8565b",
   "metadata": {},
   "source": [
    "## Experiments summary and backbone comparison\n",
    "\n",
    "Load `reports/experiments.csv` to compare different backbones and runs (ResNet-50 vs EfficientNetB0, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188bf9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "experiments_path = Path('../reports/experiments.csv')\n",
    "if not experiments_path.exists():\n",
    "    print('No experiments.csv found at', experiments_path)\n",
    "else:\n",
    "    exp_df = pd.read_csv(experiments_path)\n",
    "    display(exp_df.sort_values(['test_auc', 'val_auc'], ascending=False).reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2ffdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple backbone vs test AUC plot (if experiments are available)\n",
    "\n",
    "if 'exp_df' in globals():\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    summary = exp_df.groupby('backbone')['test_auc'].max().reset_index()\n",
    "    plt.bar(summary['backbone'], summary['test_auc'])\n",
    "    plt.ylabel('Best Test AUC')\n",
    "    plt.xlabel('Backbone')\n",
    "    plt.title('Backbone comparison (best run per model)')\n",
    "    plt.ylim(0.0, 1.0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec12f112",
   "metadata": {},
   "source": [
    "## Grad-CAM visualizations (model explainability)\n",
    "\n",
    "In this section we generate Grad-CAM heatmaps for a few test images to see where the model is focusing when predicting Parasitized vs Uninfected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae10f7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.gradcam import make_gradcam_heatmap, overlay_heatmap\n",
    "\n",
    "# Utility to grab a few images and labels from the test dataset\n",
    "sample_images = []\n",
    "sample_labels = []\n",
    "for batch_imgs, batch_labels in test_ds.take(1):\n",
    "    sample_images = batch_imgs.numpy()\n",
    "    sample_labels = batch_labels.numpy().reshape(-1).astype(int)\n",
    "\n",
    "print(\"Sample batch shape:\", sample_images.shape, sample_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7cc5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and plot Grad-CAM for a few samples\n",
    "\n",
    "# Name of the last conv layer in ResNet50 base (Keras default)\n",
    "LAST_CONV_LAYER_NAME = 'conv5_block3_out'\n",
    "\n",
    "num_to_show = min(6, len(sample_images))\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(num_to_show):\n",
    "    img = sample_images[i]\n",
    "    label = sample_labels[i]\n",
    "\n",
    "    # Our model expects preprocessed inputs; test_ds already passed through preprocess_input\n",
    "    # so we directly feed `img` to Grad-CAM.\n",
    "    heatmap = make_gradcam_heatmap(img[None, ...], model, LAST_CONV_LAYER_NAME, pred_index=0)\n",
    "\n",
    "    # Convert to displayable RGB image (0-255)\n",
    "    disp_img = np.clip((img + 1.0) * 127.5, 0, 255).astype(\"uint8\") if img.max() <= 1.1 else img.astype(\"uint8\")\n",
    "    overlay = overlay_heatmap(heatmap, disp_img, alpha=0.4)\n",
    "\n",
    "    plt.subplot(2, num_to_show // 2, i + 1)\n",
    "    plt.imshow(overlay)\n",
    "    plt.axis('off')\n",
    "    plt.title('Label: ' + ('Parasitized' if label == 0 else 'Uninfected'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
